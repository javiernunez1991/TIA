{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Cz0q-JXvfYt"
      },
      "source": [
        "## DQN sobre ambientes de Classic Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSO0NzhhvfYv"
      },
      "source": [
        "https://gymnasium.farama.org/environments/classic_control/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc6t9etEt9I2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install pyvirtualdisplay -q\n",
        "\n",
        "# Comienzo por levantar los archivos .py asociados\n",
        "!git clone https://github.com/javiernunez1991/TIA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CkxBNLp1Yiw",
        "outputId": "f9c4b2f5-94dd-4129-cae5-9f2967db2acf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[atari]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.4.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-0.2.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.4)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2024.6.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=bde73d7fe92fd6c087f3d1bdb82c7ba3b31757bf56b8047ecf9931e308d23233\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n",
            "Cloning into 'TIA'...\n",
            "remote: Enumerating objects: 409, done.\u001b[K\n",
            "remote: Counting objects: 100% (239/239), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 409 (delta 164), reused 206 (delta 142), pack-reused 170\u001b[K\n",
            "Receiving objects: 100% (409/409), 969.47 KiB | 3.04 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cwHCw6PMt9I3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import numpy as np\n",
        "import gymnasium\n",
        "import os\n",
        "os.chdir('/content/TIA/Obligatorio')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySRyzNz8t9I3"
      },
      "source": [
        "### Seteamos los devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zcyB176t9I3",
        "outputId": "93f8d9ed-f21c-4ba4-afae-e3858b024c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda:0\n",
            "Cuda Available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Running on {DEVICE}\")\n",
        "print(\"Cuda Available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcfjdDuQt9I4"
      },
      "source": [
        "### Seteo de seeds\n",
        "Siempre es buena práctica hacer el seteo de seeds para la reproducibilidad de los experimentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bxW_5r15t9I5"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bYVG_TKt9I5"
      },
      "source": [
        "### Creamos el ambiente y probamos algunas de sus funciones.\n",
        "\n",
        "En este caso elegimos el CartPole pero pueden cambiarlo en la variable *ENV_NAME*.\n",
        "El ambiente CartPole tiene la ventaja de que las recompensas son positivas y es mas fácil propagar estas hacia los estados iniciales. Mountain Car tiene como recompensa -1 por cada paso que damos y esta limitado a 200 pasos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loVxQPrwt9I5",
        "outputId": "f3fc15cd-8481-4349-cd2c-dde502a7459d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Actions: Discrete(2)\n",
            "Observation_Space: (4,)\n",
            "Next_state shape: (4,), Reward: 1.0, Terminated: False, Info: {}\n"
          ]
        }
      ],
      "source": [
        "ENVS = [\"MountainCar-v0\", \"CartPole-v1\"]\n",
        "ENV_NAME = ENVS[1]\n",
        "\n",
        "env = gymnasium.make(ENV_NAME, render_mode=\"rgb_array\")\n",
        "\n",
        "print(f\"# Actions: {env.action_space}\")\n",
        "print(f\"Observation_Space: {env.observation_space.shape}\")\n",
        "env.reset()\n",
        "next_state, reward, terminated, truncated, info = env.step(action=0)\n",
        "\n",
        "print(f\"Next_state shape: {next_state.shape}, Reward: {reward}, Terminated: {terminated}, Info: {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKJWTET1vfYz"
      },
      "source": [
        "### Seteamos los hyperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oegpMg25t9I9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def process_state(obs, device):\n",
        "    return torch.tensor(obs, device=device).unsqueeze(0)\n",
        "\n",
        "#Hiperparámetros de entrenamiento del agente DQN\n",
        "TOTAL_STEPS = 1_000_000\n",
        "EPISODES = 1500\n",
        "STEPS = 200\n",
        "\n",
        "EPSILON_INI = 1\n",
        "EPSILON_MIN = 0.1\n",
        "EPSILON_DECAY = (EPSILON_INI - EPSILON_MIN) / STEPS\n",
        "EPISODE_BLOCK = 20\n",
        "EPSILON_TIME = 100_000\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 10_000\n",
        "\n",
        "GAMMA = 0.999\n",
        "LEARNING_RATE = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y96iN1trvfYz"
      },
      "source": [
        "### Creamos el ambiente que vamos a estar usando para el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BsTl-pFqt10b",
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054b1538-1c0e-4e55-cb37-2a6cabc2c22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dim: 4, Output dim: 2\n"
          ]
        }
      ],
      "source": [
        "env = gymnasium.make(ENV_NAME)\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "\n",
        "print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HndmtfOcvfY0"
      },
      "source": [
        "### Definimos nuestra red que vamos a usar como función de aproximación para el aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tY8ItsYuvfY0"
      },
      "outputs": [],
      "source": [
        "#from dqn_model import DQN_Model\n",
        "from dqn_cnn_model import DQN_Model\n",
        "net = DQN_Model(input_dim, output_dim).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryAB4DhnvfY0"
      },
      "source": [
        "### Creamos el agente con los hyperparámetros y la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s6W4N-4jvfY0"
      },
      "outputs": [],
      "source": [
        "from dqn_agent import DQNAgent\n",
        "agent = DQNAgent(env, net, process_state, BUFFER_SIZE, BATCH_SIZE,\n",
        "                LEARNING_RATE, GAMMA, epsilon_i= EPSILON_INI,\n",
        "                epsilon_f=EPSILON_MIN, epsilon_anneal_time=EPSILON_TIME,\n",
        "                epsilon_decay = EPSILON_DECAY, episode_block = EPISODE_BLOCK, device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csx0hiF5vfY0"
      },
      "source": [
        "### Entrenamos a nuestro agente!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yehJ9Ke8vfY0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "9e039bb8-8c38-4f45-a5e3-b1150e98ec02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 8/1500 [00:00<01:13, 20.30 episodes/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Avg. Reward 20.0 over the last 20 episodes - Epsilon 0.9145 - TotalSteps 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Index tensor must have the same number of dimensions as input tensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-618d49170479>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOTAL_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/TIA/Obligatorio/abstract_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, number_episodes, max_steps_episode, max_steps, writer_name)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# Actualizar el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/TIA/Obligatorio/dqn_agent.py\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Obetener el valor estado-accion (Q) de acuerdo a la policy net para todo elemento (estados) del minibatch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# states es un minibatch de: (batch_size x 4 x 84 x 84)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mstate_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.squeeze()#.cpu().sum().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
          ]
        }
      ],
      "source": [
        "rewards, wins = agent.train(EPISODES, STEPS, TOTAL_STEPS, writer_name = ENV_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izrvp_zxvfY1"
      },
      "source": [
        "### Graficamos las recompensas obtenidas durante el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-m9repGvfY1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "average_range = EPISODE_BLOCK\n",
        "episode_ticks = int(len(rewards) / average_range)\n",
        "\n",
        "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
        "avg_rewards = np.mean(avg_rewards, axis=1)\n",
        "\n",
        "plt.plot(range(len(avg_rewards)), avg_rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-MkPXWtvfY1"
      },
      "source": [
        "### Creamos un video para ver la performance del agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YlyUR-CvfY1"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from gymnasium.wrappers.record_video import RecordVideo\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "import io\n",
        "import base64\n",
        "\n",
        "def show_video():\n",
        "  \"\"\"\n",
        "  Utility function to enable video recording of gym environment and displaying it\n",
        "  To enable video, just do \"env = wrap_env(env)\"\"\n",
        "  \"\"\"\n",
        "  mp4list = glob.glob('./videos/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "  \"\"\"\n",
        "  Wrapper del ambiente donde definimos un Monitor que guarda la visualizacion como un archivo de video.\n",
        "  \"\"\"\n",
        "\n",
        "  #env = Monitor(env, './video', force=True)\n",
        "  env = RecordVideo(env,video_folder='./videos')\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLbjRFlSvfY1"
      },
      "outputs": [],
      "source": [
        "env = wrap_env(gymnasium.make(ENV_NAME, render_mode=\"rgb_array\"))\n",
        "observation,_ = env.reset()\n",
        "\n",
        "while True:\n",
        "    env.render()\n",
        "\n",
        "    action = agent.select_action(process_state(observation, DEVICE), train=False)\n",
        "    observation, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "    if done or truncated:\n",
        "      break\n",
        "\n",
        "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
        "env.close()\n",
        "show_video()\n",
        "\n",
        "del env"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}