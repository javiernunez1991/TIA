{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Step Actor-Critic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/classic_control/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc6t9etEt9I2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cwHCw6PMt9I3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySRyzNz8t9I3"
   },
   "source": [
    "### Seteamos los devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zcyB176t9I3",
    "outputId": "4239691d-04a7-47de-9898-ee53cf047a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n",
      "Cuda Available: True\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on {DEVICE}\")\n",
    "print(\"Cuda Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcfjdDuQt9I4"
   },
   "source": [
    "### Seteo de seeds\n",
    "Siempre es buena práctica hacer el seteo de seeds para la reproducibilidad de los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bxW_5r15t9I5"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bYVG_TKt9I5"
   },
   "source": [
    "### Creamos el ambiente y probamos algunas de sus funciones.\n",
    "\n",
    "En este caso elegimos el CartPole pero pueden cambiarlo en la variable *ENV_NAME*.\n",
    "El ambiente CartPole tiene la ventaja de que las recompensas son positivas y es mas fácil propagar estas hacia los estados iniciales. Mountain Car tiene como recompensa -1 por cada paso que damos y esta limitado a 200 pasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loVxQPrwt9I5",
    "outputId": "18b7ed97-88dd-4b1e-a2cc-b2636686bfc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions # Discrete(2)\n",
      "(4,)\n",
      "(4,),\n",
      " 1.0,\n",
      " False,\n",
      " {}\n"
     ]
    }
   ],
   "source": [
    "ENVS = [\"MountainCar-v0\", \"CartPole-v1\"]\n",
    "ENV_NAME = ENVS[1]\n",
    "\n",
    "env = gymnasium.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Actions #\",env.action_space)\n",
    "print(env.observation_space.shape)\n",
    "env.reset()\n",
    "next_state, reward, terminated, truncated, info = env.step(action=0)\n",
    "\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {terminated},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seteamos los hyperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oegpMg25t9I9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def process_state(obs, device):\n",
    "    return torch.tensor(obs, device=device).unsqueeze(0)\n",
    "\n",
    "#Hiperparámetros de entrenamiento del agente DQN\n",
    "TOTAL_STEPS = 1000000\n",
    "EPISODES = 1500\n",
    "STEPS = 200\n",
    "\n",
    "EPSILON_INI = 1\n",
    "EPSILON_MIN = 0.1\n",
    "EPSILON_DECAY = 40000\n",
    "EPISODE_BLOCK = 20\n",
    "EPSILON_TIME = 100000\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "GAMMA = 0.999\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el ambiente que vamos a estar usando para el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BsTl-pFqt10b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: 4, Output dim: 2\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(ENV_NAME)\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "print(f\"Input dim: {input_dim}, Output dim: {output_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repasamos el pesudo código del agente One-Step Actor-Critic\n",
    "![](pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos nuestras redes Actor y Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ActorCriticGeneric_Model(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(in_features=input_dim, out_features=32)\n",
    "    self.output = nn.Linear(in_features=32, out_features=output_dim)\n",
    "\n",
    "  def forward(self, env_input):\n",
    "    result = F.relu(self.fc1(env_input))\n",
    "    return self.output(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = ActorCriticGeneric_Model(input_dim, output_dim).to(DEVICE)\n",
    "critic_net = ActorCriticGeneric_Model(input_dim, 1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "\n",
    "class OneStepActorCriticAgent():\n",
    "    def __init__(self, gym_env, obs_processing_func, gamma, episode_block, actor_net, critic_net):\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        #Guardo las redes en el estado del agente\n",
    "        self.actor_net = actor_net\n",
    "        self.critic_net = critic_net\n",
    "\n",
    "\n",
    "        # Funcion phi para procesar los estados.\n",
    "        self.state_processing_function = obs_processing_func\n",
    "\n",
    "        self.env = gym_env\n",
    "        # Hyperparameters\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.episode_block = episode_block\n",
    "\n",
    "        self.total_steps = 0\n",
    "\n",
    "        # Asignar una función de costo para el Actor y Critic (y enviarlas al dispositivo adecuado)\n",
    "        # self.loss_function = ?\n",
    "\n",
    "        # Asignar optimizadores (Adam)\n",
    "        # self.optimizerActor = ?\n",
    "        # self.optimizerCritic = ?\n",
    "    \n",
    "    def train(self, number_episodes = 50000, max_steps_episode = 10000, max_steps=1000000):\n",
    "      rewards = []\n",
    "      total_steps = 0\n",
    "\n",
    "      for ep in tqdm(range(number_episodes), unit=' episodes'):\n",
    "        if total_steps > max_steps:\n",
    "            break\n",
    "        \n",
    "        # Observar estado inicial como indica el algoritmo\n",
    "\n",
    "        current_episode_reward = 0.0\n",
    "\n",
    "        for s in range(max_steps):\n",
    "\n",
    "            # Seleccionar accion usando la política del actor.\n",
    "\n",
    "            # Ejecutar la accion, observar resultado y procesarlo como indica el algoritmo.\n",
    "\n",
    "            current_episode_reward += reward\n",
    "            total_steps += 1\n",
    "\n",
    "            # Calcular la ventaja (delta)\n",
    "            \n",
    "            # Actualizar el modelo del Critic\n",
    "            \n",
    "            # Actualizar el modelo del Actor\n",
    "\n",
    "            # Actualizar el estado\n",
    "\n",
    "            done = False\n",
    "            if done: \n",
    "                break\n",
    "        \n",
    "        rewards.append(current_episode_reward)\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "\n",
    "        # Report on the traning rewards every EPISODE BLOCK episodes\n",
    "        if ep % self.episode_block == 0:\n",
    "          print(f\"Episode {ep} - Avg. Reward over the last {self.episode_block} episodes {np.mean(rewards[-self.episode_block:])} total steps {total_steps}\")\n",
    "\n",
    "      print(f\"Episode {ep + 1} - Avg. Reward over the last {self.episode_block} episodes {np.mean(rewards[-self.episode_block:])} total steps {total_steps}\")\n",
    "\n",
    "      return rewards    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el agente con los hyperparámetros y la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OneStepActorCriticAgent(env, process_state, GAMMA, EPISODE_BLOCK, actor_net, critic_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos a nuestro agente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1500 [00:00<02:48,  8.86 episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Avg. Reward over the last 20 episodes 1000000.0 total steps 1000000\n",
      "Episode 3 - Avg. Reward over the last 20 episodes 1000000.0 total steps 2000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rewards, wins = agent.train(EPISODES, STEPS, TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficamos las recompensas obtenidas durante el entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "average_range = EPISODE_BLOCK\n",
    "episode_ticks = int(len(rewards) / average_range)\n",
    "\n",
    "avg_rewards = np.array(rewards).reshape((episode_ticks, average_range))\n",
    "avg_rewards = np.mean(avg_rewards, axis=1)\n",
    "\n",
    "plt.plot(range(len(avg_rewards)), avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos un video para ver la performance del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import io\n",
    "import base64\n",
    "\n",
    "def show_video():\n",
    "  \"\"\"\n",
    "  Utility function to enable video recording of gym environment and displaying it\n",
    "  To enable video, just do \"env = wrap_env(env)\"\"\n",
    "  \"\"\"\n",
    "  mp4list = glob.glob('./videos/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else:\n",
    "    print(\"Could not find video\")\n",
    "\n",
    "\n",
    "def wrap_env(env):\n",
    "  \"\"\"\n",
    "  Wrapper del ambiente donde definimos un Monitor que guarda la visualizacion como un archivo de video.\n",
    "  \"\"\"\n",
    "\n",
    "  #env = Monitor(env, './video', force=True)\n",
    "  env = RecordVideo(env,video_folder='./videos')\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap_env(gymnasium.make(ENV_NAME, render_mode=\"rgb_array\"))\n",
    "observation,_ = env.reset()\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    action = agent.select_action(process_state(observation, DEVICE), train=False)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    if done or truncated:\n",
    "      break\n",
    "\n",
    "# Cerramos la conexion con el Monitor de ambiente y mostramos el video.\n",
    "env.close()\n",
    "show_video()\n",
    "\n",
    "del env"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
